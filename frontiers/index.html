<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet"> 
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<script src="http://d3js.org/d3.v3.min.js"></script>
	<script src="http://labratrevenge.com/d3-tip/javascripts/d3.tip.v0.6.3.js"></script>
	<script src="graph-scroll.js"></script>
	<script src="d3v4+jetpack.js"></script>
  </head>
  <body>
  	<div style="overflow: hidden;">
	  	<div class="intro-text" style="float: left;">
		  	<h1> Fairness in Machine Learning </h1>
		  	<div class="introduction"> 
		  		ProPublica's 2016 piece, <i>Machine Bias</i>, started a conversation on algorithmic accountability and fairness across the country. The story explored the workings of Northpointe's COMPAS - a risk assesment software most popularly used nationwide. ProPublica found that black defendants were incorrectly judged to be at a higher risk of recidivism compared to while defendents. The article generated a lot of interest in fairness and much research has concluded that there are six types of fairness, some of which are incompatible with each other and with accuracy. This project attempts to simplify and visualize this research.
		  	</div>
		</div>
	</div>

	<br>
	<br>
	<br>
	<br>
	<br>

  	<div class="container-1" id="container">
  		<div id="graph1">
			<img id="head-image" src="starter-image.png">
  		</div>
  		<div id="sections">
		  	<div class="information">
				<b>Introduction</b>
				<br>
				In many industries, ranging from criminal justice to medicine, algorithms perform the risk assesment to decide the consequences of decisions and actions. As the scope of such algorithms increases, journalists and academics have voiced concerns that these models might be encoding human biases, as they are built on data that reflect human biases in past decisions. And outcomes.
			</div>
			<br>
			<div class="information">
				Over the last couple of years, the research community has proposed formal and mathematical definitions of fairness to guide the conversation and design around equitable risk assesment tools. These definitions are intuitive, but each of these definitions of fairness has its own limitations, and are not the best measures of detecting discriminatory algorithms. In fact, designing algorithms that satisfy these definitions of fairness can end up negatively impacting minority (as well as majority) community well-being.
			</div>
			<br>
			<div class="information">
				<b>What is Fairness?</b>
				<br>
				Fairness, as defined by Barocas and Hardt, is an unjustified basis for differentiation. Historically, this basis has had practical irrelevance in relation to the task being performed.
			</div>
			<br>
			<br>
			<div class="information" style="color:red">
				Think gender, race, or sexual preference.
			</div>
			<br>
			<br>
			<div class="information">
				Even if statistically relevant, there are certain factors considered morally irrelevant, as a decision that we consciously take and choose not to differentiate on the basis of.
			</div>
			<br>
			<br>
			<div class="information" style="color:red">
				Think of physical disabilities, or age.
			</div>
			<br>
			<br>
			<div class="information">
				There are two doctrines of discrimination law:
				<ul>
					<li>Disparate Treatment: This is the idea that if your model even considers certain factors (whether or not they are used in your model) then it is illegal. Sometimes, these factors can be a matter of formality, like asking for age and gender on any form. Disparate treatment also occurs when proxies are used for these factors, like redlining districts where African Americans are in majority.</li>
					<br>
					<li>Disparate Impact: This doctrine avoids using any factors that can cause discrimination - instead, it focuses on facially neutral attributes. However, disparity still manages to appear in the output. This leads to questions like is the process justified, and is systemic bias even avoidable?</li>
				</ul>
			</div>
			<br>
			<div class="information">
				As you can see, fairness is intrinsically tied with discrimination, which is not a general concept - it often depends on context and a domain. However, it often concerns important decisions that affect life situations. 
			</div>
		</div>
	</div>

	<div class="information">
		<b>Definitions of Fairness</b>
		<br>
		So then, how does one go about defining the concept of fairness? Extensive research has been done on the topic, and certain definitions have been widely accepted to quantify fairness and discrimination.
		<br>
		<br>
		From Disparate Treatment comes Anti-Classification fairness, where algorithms do not consider protected characteristics like race, gender and their proxies. Classification parity requires that certain measures of predictive performance be equal across all groups defined by these protected attributes. Calibration, which ties in with the concept of independence, requires that outcomes be independent of the protected attributes, after controlling for estimated risk.
		<br>
		<br>
		Berk et al define fairness through simple mathematical concepts. For this, let us consider a simple construct: we want to predict whether a person will like coffee with or without sugar. We employ a tool to predict this in a binary manner. Even if we were to design a continuous algorithm that calculated the probablity of a person wanting coffee with sugar, we would have to set a threshold value - if the algorithm predicted a value above the threshold, we would say they prefer coffee with coffee and vice versa.
		<br>
		<br>
		Let us assume our tool considers the following attributes of the person: <b>how often they buy ice cream</b>, <b>how many hours they sleep</b>, <b>whether they have eaten before the coffee</b> and <b>the brand of shoes they are wearing</b>.
	</div>

	<div class="container-2" id="container">
  		<div id="graph2">
  		</div>
  		<div id="sections">
			<div class="information">
				Let us say we start out with 100 people, half who drink coffee with sugar, and half without. We create a confusion matrix like the one to the right.
			</div>
			<br>
			<div class="information">
				The columns represent the actual number of people in each category, positive being the ones that take coffee with sugar and negative are the ones without.
			</div>
			<div class="information">
				On the other hand, the rows represent the predictions of the algorithm about the number of people in each category, positive being the ones that take coffee with sugar and negative are the ones without.
			</div>
			<br>
			<div class="information">
				We shorten the true positives, false positives, false negatives and true negatives to TP, FP, FN and TN respectively. The confusion matrix makes it very easy to record certain simple statistics:
				<ul>
					<li>
						Sample size: This is the total number of observations, denoted by <i>N</i>, and is the sum of all four cells in the confusion matrix. So, <i>N</i> = TP + FP + FN + TN
					</li>
					<br>
					<li>
						Base Rate: This is the proportion of actual successes, or actual failures, from the total sample. The choice between success and failure depends on the experimenter. This is defined as (TP + FN)/<i>N</i> or (TP + FN)/(TP + FP + FN + TN), or (TP + FN)/<i>N</i>.
					</li>
					<br>
					<li>
						Prediction Distribution: The proportion of predictions predicted to fail, and the proportion predicted to succeed. This translates to (FN + TN)/<i>N</i> and (TP + FP)/<i>N</i> respectively.
					</li>
					<br>
					<li>
						Overall Procedure Error: This is the proportion of cases that are misclassified by our algorithm, (FP + FN)/(TP + FP + FN + TN). The complement to this is overall procedure accuracy, defined as (TP + TN)/(TP + FP + FN + TN).
					</li>
					<br>
					<li>
						Conditional Procedure Error: This is the proportion of cases misclassified <i>conditional on one of the two actual outcomes</i>. This gives us the <i> false positive rate</i>, FP/(FP + TN) or the <i>false negative rate</i>, FN/(TP + FN).
					</li>
					<br>
					<li>
						Conditional Use Error: This is the proportion of cases misclassified <i> conditional on one of the two predicted outcomes</i>. The incorrect failure predictions is FN/(TN + FN) and the incorrect success prediction proportion is FP/(TP + FP).
					</li>
				</ul>
			</div>
			<br>
			<div class="information">
				Fairness can be defined in <i>six</i> different ways, according to the State of the Art paper on Fairness Criminal Risk Assesments. Each of these definitions can further be represented using features of the confusion matrix, and the above defined concepts. The exact features that we use, however, will change depending on the type of fairness under consideration. 
			</div>
			<br>
			<div class="information">
				The confusion matrix, however, makes it easy to see that different kinds of fairness are not only related to each other, but also with accuracy. It also simplifies the notion that total fairness cannot be achieved simultaneously.
			</div>
			<div class="information">
				Coming back to our example of classifying people who like their sugar with coffee, we can see that the columns should add upto 50, since our population is split between people who like coffee with and without sugar. The sum of the rows, however, cannot be fixed within the population, and will depend on how our classifier algorithm is defined. 
			</div>
			<div class="information">
				Keep in mind, a confusion matrix can also be created for a subset of our sample. For example, we can have a confusion matrix for people who sleep less than five hours a day, and another for people who sleep more. We now define the different types of fairness, trying to balance fairness of our algorithm across both these (or any other) groups created on the basis of sleeping patterns.
			</div>
		</div>
	</div>

	<div class="container-3" id="container">
		<div id="graph3">
		</div>
		<div id="sections">
			<div class="information">
				Overall Accuracy Equality
				<br>
				When the overall procedure accuracy is the same for each group, overall accuracy equality is achieved. Overall procedure accuracy, as previously mentioned, is the total correct number of predictions made by the algorithm, and is defined as (TP + TN)/<i>N</i>.
				<br>
				<br>
				This definition assumes that true negatives and true positives are equally desirable in our system. In real-world cases, however, it is possible that true positives may be more desirable than true negatives. Rather, false negatives (wrong predictions of negatives) might be twice as undesirable as false positives (wrong predictions of positives).
				<br>
				<br>
				Overall Accuracy Equality, according to the State of the Art paper, is not commonly used as it doesn't differentiate between success and failure accuracy.
			</div>
			<br>
			<div class="information">
				Statistical Parity
				<br>
				Statistical Parity is achieved when the prediction distribution is the same across all protected groups. From our previous example, this implies that if sleep patterns was a protected class, the proportion of people predicted to have coffee with (or without) sugar should be equal for people who get less than five hours of sleep, and for people who get more than five hours of sleep.
				<br>
				<br>
				Statistical parity, also known as demographic parity, has been criticized as it can lead to highly undesirable outcomes. It is defined as (TP + FP)/<i>N</i> or (FN + TN)/<i>N</i>.
				<br>
				<br>
				By simply sampling more (or less) people from these predicted groups, we could modify the proportions for prediction distribution and achieve statistical parity by changing our population, rather than modifying our algorithm, to equalize the proportion of predictions based on sleep patterns.
			</div>
			<div class="information">
				Conditional Procedure Accuracy Equality
				<br>
				As the name suggests, this fairness is achieved when the conditional procedure accuracy is the same across all protected groups. Simply put, the accuracy of predicting positives (and negatives) from all actual positives (and negatives) should be equal across groups. Thus, the value for TP/(TP + FN) and TN/(TN + FP) should be the same. 
				This is similar to considering that the false positive and false negative rates are the same across groups. 
				<br>
				<br>
				A closely similar definition of conditional procedure accuracy equality is also referred to as "equalized odds," and "equality of opportunity" is the same as this, for the more desirable outcome only.
			</div>
			<div class="information">
				Conditional Use Accuracy Equality
				<br>
				This differs from the previous fairness in that it depends on the ratio of a predicted outcome among all predictions. The proportion of each prediction should remain the same across groups. This fairness asks that conditional on the prediction of success (or failure), is the probability of success (or failure) the same across groups? Thus, the ratios TP/(TP + FP) and TN/(FN + TN) should be equal across all groups.
				<br>
				<br>
				Both conditional procedure and conditional use accuracy equality are concerns in criminal justive risk assesments (more on this later). Chouldechova refers to conditional use accuracy equality as positive predictive value (PPV) and corresponds to TP/(TP + FN).
			</div>
			<div class="information">
				Treatment Equality
				<br>
				Treatment equality is achieved when the ratio of false negatives to false positives (or vice versa) is the same across protected groups. This equality is considered a "lever with which to achieve other kinds of fairness." It is defined as FN/FP or FP/FN.
				<br>
				<br>
				We could treat people who get more sleep differently than people who get lesser sleep to achieve conditional use accuracy equality, by weighing their false negatives for heavily. This would change the treatment ratio, and would be an indicator of unfair treatment of people who get more seep.
			</div>
			<div class="information">
				Total Fairness
				<br>
				Total fairness is achieved when all the above fairness, overall accuracy equality, statistical parity, conditional procedure accuracy equality, conditional use accuracy equality and treatment equality are achieved. 
				<br>
				<br>
				Recent research shows that some of the above fairness are at odds at each other, thus making it impossible to achieve total fairness.
			</div>
			<div class="information">
				Again, it is important to remember that definitions of fairness exists only between different groups (or classes) of the sample, and when there are more than two outcome categories. 
			</div>
		</div>
	</div>
	<br>
	<br>
	<br>
	<br>
	<div id="interactive" class="interactive">
	</div>
	<div id="interactive2" class="interactive">
	</div>
	<script src="animation.js"></script>
	<script src="interactive.js"></script>
  	<script src="interactive2.js"></script>
  </body>
</html>