<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet"> 
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<script src="graph-scroll.js"></script>
	<script src="d3v4+jetpack.js"></script>
  </head>
  <body>
  	<div style="overflow: hidden;">
	  	<div class="intro-text" style="float: left;">
		  	<h1> Fairness in Machine Learning </h1>
		  	<div class="introduction"> 
		  		ProPublica's 2016 piece, <i>Machine Bias</i>, started a conversation on algorithmic accountability and fairness across the country. The story explored the workings of Northpointe's COMPAS - a risk assesment software most popularly used nationwide. ProPublica found that black defendants were incorrectly judged to be at a higher risk of recidivism compared to while defendents. The article generated a lot of interest in fairness and much research has concluded that there are six types of fairness, some of which are incompatible with each other and with accuracy. This project attempts to simplify and visualize this research.
		  	</div>
		</div>
	</div>

	<br>
	<br>
	<br>
	<br>
	<br>

  	<div class="container-1" id="container">
  		<div id="graph1">
			<img id="head-image" src="starter-image.png">
  		</div>
  		<div id="sections">
		  	<div class="information">
				<b>Introduction</b>
				<br>
				In many industries, ranging from criminal justice to medicine, algorithms perform the risk assesment to decide the consequences of decisions and actions. As the scope of such algorithms increases, journalists and academics have voiced concerns that these models might be encoding human biases, as they are built on data that reflect human biases in past decisions. And outcomes.
			</div>
			<br>
			<div class="information">
				Over the last couple of years, the research community has proposed formal and mathematical definitions of fairness to guide the conversation and design around equitable risk assesment tools. These definitions are intuitive, but each of these definitions of fairness has its own limitations, and are not the best measures of detecting discriminatory algorithms. In fact, designing algorithms that satisfy these definitions of fairness can end up negatively impacting minority (as well as majority) community well-being.
			</div>
			<br>
			<div class="information">
				<b>What is Fairness?</b>
				<br>
				Fairness, as defined by Barocas and Hardt, is an unjustified basis for differentiation. Historically, this basis has had practical irrelevance in relation to the task being performed.
			</div>
			<br>
			<br>
			<div class="information" style="color:red">
				Think gender, race, or sexual preference.
			</div>
			<br>
			<br>
			<div class="information">
				Even if statistically relevant, there are certain factors considered morally irrelevant, as a decision that we consciously take and choose not to differentiate on the basis of.
			</div>
			<br>
			<br>
			<div class="information" style="color:red">
				Think of physical disabilities, or age.
			</div>
			<br>
			<br>
			<div class="information">
				There are two doctrines of discrimination law:
				<ul>
					<li>Disparate Treatment: This is the idea that if your model even considers certain factors (whether or not they are used in your model) then it is illegal. Sometimes, these factors can be a matter of formality, like asking for age and gender on any form. Disparate treatment also occurs when proxies are used for these factors, like redlining districts where African Americans are in majority.</li>
					<br>
					<li>Disparate Impact: This doctrine avoids using any factors that can cause discrimination - instead, it focuses on facially neutral attributes. However, disparity still manages to appear in the output. This leads to questions like is the process justified, and is systemic bias even avoidable?</li>
				</ul>
			</div>
			<br>
			<div class="information">
				As you can see, fairness is intrinsically tied with discrimination, which is not a general concept - it often depends on context and a domain. However, it often concerns important decisions that affect life situations. 
			</div>
		</div>
	</div>

	<div class="information">
		<b>Definitions of Fairness</b>
		<br>
		So then, how does one go about defining the concept of fairness? Extensive research has been done on the topic, and certain definitions have been widely accepted to quantify fairness and discrimination.
		<br>
		<br>
		From Disparate Treatment comes Anti-Classification fairness, where algorithms do not consider protected characteristics like race, gender and their proxies. Classification parity requires that certain measures of predictive performance be equal across all groups defined by these protected attributes. Calibration, which ties in with the concept of independence, requires that outcomes be independent of the protected attributes, after controlling for estimated risk.
		<br>
		<br>
		Berk et al define fairness through simple mathematical concepts. For this, let us consider a simple construct: we want to predict whether a person will like coffee with or without sugar. We employ a tool to predict this in a binary manner. Even if we were to design a continuous algorithm that calculated the probablity of a person wanting coffee with sugar, we would have to set a threshold value - if the algorithm predicted a value above the threshold, we would say they prefer coffee with coffee and vice versa.
		<br>
		<br>
		Let us assume our tool considers the following attributes of the person: <b>how often they buy ice cream</b>, <b>how many hours they sleep</b>, <b>whether they have eaten before the coffee</b> and <b>the brand of shoes they are wearing</b>.
	</div>

	<div class="container-2" id="container">
  		<div id="graph2">
  		</div>
  		<div id="sections">
			<div class="information">
				Let us say we start out with 100 people, half who drink coffee with sugar, and half without. We create a confusion matrix like the one to the right.
			</div>
			<br>
			<div class="information">
				The columns represent the actual number of people in each category, positive being the ones that take coffee with sugar and negative are the ones without, while the rows represent the prediction of our algorithm. 
			</div>
			<br>
			<div class="information">
				We shorten the true positives, false positives, false negatives and true negatives to TP, FP, FN and TN respectively. The confusion matrix makes it very easy to record certain simple statistics:
				<ul>
					<li>
						Sample size: This is the total number of observations, denoted by <i>N</i>, and is the sum of all four cells in the confusion matrix. So, <i>N</i> = TP + FP + FN + TN
					</li>
					<br>
					<li>
						Base Rate: This is the proportion of actual successes, or actual failures, from the total sample. The choice between success and failure depends on the experimenter. This is defined as (TP + FN)/<i>N</i> or (TP + FN)/(TP + FP + FN + TN), or (TP + FN)/<i>N</i>.
					</li>
					<br>
					<li>
						Prediction Distribution: The proportion of predictions predicted to fail, and the proportion predicted to succeed. This translates to (FN + TN)/<i>N</i> and (TP + FP)/<i>N</i> respectively.
					</li>
					<br>
					<li>
						Overall Procedure Error: This is the proportion of cases that are misclassified by our algorithm, (FP + FN)/(TP + FP + FN + TN). The complement to this is overall procedure accuracy, defined as (TP + TN)/(TP + FP + FN + TN).
					</li>
					<br>
					<li>
						Conditional Procedure Error: This is the proportion of cases misclassified <i>conditional on one of the two actual outcomes</i>. This gives us the <i> false positive rate</i>, FP/(FP + TN) or the <i>false negative rate</i>, FN/(TP + FN).
					</li>
					<br>
					<li>
						Conditional Use Error: This is the proportion of cases misclassified <i> conditional on one of the two predicted outcomes</i>. The incorrect failure predictions is FN/(TN + FN) and the incorrect success prediction proportion is FP/(TP + FP).
					</li>
				</ul>
			</div>
			<br>
			<div class="information">
				Fairness can be defined in <i>six</i> different ways, according to the State of the Art paper on Fairness Criminal Risk Assesments. Each of these definitions can further be represented using features of the confusion matrix, and the above defined concepts. The exact features that we use, however, will change depending on the type of fairness under consideration. 
			</div>
			<br>
			<div class="information">
				The confusion matrix, however, makes it easy to see that different kinds of fairness are not only related to each other, but also with accuracy. It also simplifies the notion that total fairness cannot be achieved simultaneously.
			</div>
			<div class="information">
				Coming back to our example of classifying people who like their sugar with coffee, we can see that the columns should add upto 50, since our population is split between people who like coffee with and without sugar. The sum of the rows, however, cannot be fixed within the population, and will depend on how our classifier algorithm is defined. 
			</div>
			<div class="information">
				Keep in mind, a confusion matrix can also be created for a subset of our sample. For example, we can have a confusion matrix for people who sleep less than five hours a day, and another for people who sleep more. We now define the different types of fairness, trying to balance fairness of our algorithm across both these (or any other) groups created on the basis of sleeping patterns.
			</div>
		</div>
	</div>

	<div class="container-3" id="container">
		<div id="graph3">
		</div>
		<div id="sections">
			<div class="information">
				Overall Accuracy Equality
				<br>
				When the overall procedure accuracy is the same for each group, overall accuracy equality is achieved. Overall procedure accuracy, as previously mentioned, is the total correct number of predictions made by the algorithm, and is defined as (TP + TN)/<i>N</i>.
				<br>
				<br>
				This definition assumes that true negatives and true positives are equally desirable in our system. In real-world cases, however, it is possible that true positives may be more desirable than true negatives. Rather, false negatives (wrong predictions of negatives) might be twice as undesirable as false positives (wrong predictions of positives).
				<br>
				<br>
				Overall Accuracy Equality, according to the State of the Art paper, is not commonly used as it doesn't differentiate between success and failure accuracy.
			</div>
			<br>
			<div class="information">
				Statistical Parity
				<br>
				Statistical Parity is achieved when the prediction distribution is the same across all protected groups. From our previous example, this implies that if sleep patterns was a protected class, the proportion of people predicted to have coffee with (or without) sugar should be equal for people who get less than five hours of sleep, and for people who get more than five hours of sleep.
				<br>
				<br>
				Statistical parity, also known as demographic parity, has been criticized as it can lead to highly undesirable outcomes. 
				<br>
				<br>
				By simply sampling more (or less) people from these predicted groups, we could modify the proportions for prediction distribution and achieve statistical parity by changing our population, rather than modifying our algorithm, to equalize the proportion of predictions based on sleep patterns.
			</div>
			<div class="information">
				Conditional Procedure Accuracy Equality
				<br>
				As the name suggests, this fairness is achieved when the conditional procedure accuracy is the same across all protected groups. Simply put, the accuracy of predicting positives (and negatives) from all actual positives (and negatives) should be equal across groups. Thus, the value for TP/(TP + FN) and TN/(TN + FP) should be the same. 
				This is similar to considering that the false positive and false negative rates are the same across groups. 
				<br>
				<br>
				A closely similar definition of conditional procedure accuracy equality is also referred to as "equalized odds," and "equality of opportunity" is the same as this, for the more desirable outcome only.
			</div>
			<div class="information">
				Conditional Use Accuracy Equality
				<br>
				This differs from the previous fairness in that it depends on the ratio of a predicted outcome among all predictions. The proportion of each prediction should remain the same across groups. This fairness asks that conditional on the prediction of success (or failure), is the probability of success (or failure) the same across groups? Thus, the ratios TP/(TP + FP) and TN/(FN + TN) should be equal across all groups.
				<br>
				<br>
				Both conditional procedure and conditional use accuracy equality are concerns in criminal justive risk assesments (more on this later). Chouldechova refers to conditional use accuracy equality as positive predictive value (PPV) and corresponds to TP/(TP + FN).
			</div>
			<div class="information">
				Treatment Equality
				<br>
				Treatment equality is achieved when the ratio of false negatives to false positives (or vice versa) is the same across protected groups. This equality is considered a "lever with which to achieve other kinds of fairness." We could treat people who get more sleep differently than people who get lesser sleep to achieve conditional use accuracy equality, by weighing their false negatives for heavily. This would change the treatment ratio, and would be an indicator of unfair treatment of people who get more seep.
			</div>
			<div class="information">
				Total Fairness
				<br>
				Total fairness is achieved when all the above fairness, overall accuracy equality, statistical parity, conditional procedure accuracy equality, conditional use accuracy equality and treatment equality are achieved. 
				<br>
				<br>
				Recent research shows that some of the above fairness are at odds at each other, thus making it impossible to achieve total fairness.
			</div>
			<div class="information">
				Again, it is important to remember that definitions of fairness exists only between different groups (or classes) of the sample, and when there are more than two outcome categories. 
			</div>
		</div>
	</div>

	<div class="information">
		Another way to look at fairness and discrimination is through the different variables that are involved in our model. A popular framework that forms the basis of most of our previous discussion looks at the relationship between our protected groups (A: a, b), our target variable (C) and some other variable (R). Using these three, we can define
		<ul>
			<li>
				Independence: Our target variable C has to be independent of our protected groups A. For two groups, a and b, we should have P_a(C=c) and P_b(C=c) to be equal, meaning that the probability of two people being classified as people who like coffee with sugar is independent of the brand of shoe they wear.
			</li>
			<li>
				Separation: 
			</li>
			<li>
				Sufficiency
			</li>
		</ul>
	</div>
	<br>
	<br>
	<br>
	<br>
	<div class="information">
		Ei vim accusam torquatos, cu per populo feugait. In verear nostrud quo, eos sale ponderum eleifend ad, vim ex oportere facilisis. Sea eu assum eloquentiam, quot atomorum vis cu, eu audire dolorum iudicabit est. Ad nusquam posidonium has, sit ornatus iracundia et. Vim ut tota habemus, sea stet debet utinam ea. Aeque accusamus eu nec.

		Vidit aeterno scripserit has ne. No per vocent integre molestie, id duo magna electram pertinacia, ne pri dicam partem probatus. Vel eu ignota possit periculis, mea graece singulis no. Odio vero disputationi vim cu. Ad porro delicata sit, cu has detracto vituperata repudiandae.

		Legimus recusabo usu ad, adipisci sensibus ad ius. Animal adipisci et eos. No graeci aperiri consetetur eos. Ubique ignota honestatis his cu, ad his dolore persius epicurei, inani minimum per id. Sit ut hendrerit neglegentur, in ignota tincidunt eum, per denique salutatus referrentur at. Vim debet vivendum te, perpetua adipiscing adversarium nec eu.

		Et voluptua partiendo sit, mel mucius tibique antiopam ex. Harum gloriatur intellegam id per, ne alia stet vivendum mea, ne putant invidunt inciderint vel. Est causae dissentiunt et, ex antiopam consectetuer cum. No volumus nostrum pro, at stet quaeque sed. Prompta dolorem ut vis, possit apeirian scripserit no mea.
	</div>
	<br>
	<div class="information">
		Equidem pertinacia argumentum cu per, ei elit clita vel. Eum cu dicam assentior. Option tritani definitiones te mea. Sanctus labores id has, diam liber vis eu, imperdiet molestiae at ius. His te stet adolescens, pri fuisset referrentur ut. At est idque suscipit adolescens, cum ut tollit salutandi voluptatum.

		An mea illum fabellas, in nec equidem ponderum. Quem contentiones ei sed. Eos ea eruditi imperdiet, deleniti hendrerit scriptorem vel in. Et modus nominati mel, ei his aperiri conceptam. Est adhuc cetero ea, vel ea inani placerat voluptaria. Brute error pri cu, eos ad soluta tamquam nominati.

		Ei volumus detracto nam, ex detracto constituto pri. Clita ridens vix cu, possim eruditi reprimique nam ad. Ius ad unum semper minimum, iudico numquam ius et. Ut eos tota rationibus, ius id eirmod accommodare. Te vix accusam facilisi.
	</div>
	<div class="information">
		Vix elit meliore in. Vix facete nostrud ad, duo putent abhorreant cu. An eius vulputate sea. Ea postulant referrentur eum, et vim libris dolorem quaerendum, sed eu debet adipiscing definitiones. Aperiam dissentiet te mea, ut duo minim errem iracundia, magna abhorreant vis ut. Has eius meliore percipitur ei, ut choro laboramus quo, ea viderer vivendo vix. Pri ut modus aeterno tacimates, brute prompta ex usu.

		Sit nominavi probatus vituperatoribus ad, ea mei veri aperiam. Cetero percipit sed in, vis mucius ancillae assentior et, cum option aliquip singulis id. An rebum denique eam, vel an tempor aeterno. Ullum labores volumus ei has, at vim sint mutat, postea platonem et ius. Ea movet democritum theophrastus mea, no possit audire ocurreret sea.

		His ut dictas delicata, vivendum consulatu eam at. An mel quem vide voluptatibus, an pri minimum consequat. At eius paulo has. Ius no brute docendi. Vix sensibus perpetua urbanitas cu. No est admodum deseruisse.
	</div>
	<div class="information">
		Ut quo impedit tractatos consulatu, et nam quis euripidis. Iusto iudico mnesarchum duo ad. Docendi mediocrem disputationi an sea, illud ullum his ea. Qui mundi nihil euismod eu, at aliquid delectus dissentias est, tantas euripidis per ne.

		An sanctus delectus perpetua sed. Referrentur instructior eos at. Simul propriae platonem ut quo, et vim primis civibus, no movet simul mea. Vide expetenda no sea, ei brute partem prompta vix. Ne viris tempor atomorum cum, id qui purto interesset.

		Epicuri explicari ex mel, homero cotidieque complectitur vix ad. Sit iisque fabellas eu. Ex nam everti moderatius, est mucius insolens recusabo ad. Ad liber senserit efficiantur mel. Ei per illum viris.
	</div>
	<div class="information">
		Duo quando numquam ei. Ei sed debet iuvaret salutatus, sonet accusamus cu vel. Nostro postulant hendrerit nam ex. Cu feugait percipit instructior has, qui diam veri eu. Eum ad malorum electram iracundia, ex doctus patrioque comprehensam mea. Posse clita incorrupte ne per, per te aliquando instructior.

		Lorem minim et est, adhuc adversarium ullamcorper vis et, fuisset honestatis appellantur has id. Dicta tractatos ei his, epicurei intellegam no nam, eos facer menandri in. Te euismod dissentias quaerendum sit, principes interpretaris in cum, nam ad nonumes pericula qualisque. Qui augue eirmod an, et pro dolorem insolens erroribus.

		Sed mnesarchum posidonium eu, partem tibique disputando an pri, mei fabellas voluptatum et. No aeterno minimum deleniti has, iusto albucius pro at. Ut dicta evertitur conceptam vim. Ut quo dolor accusam, dicta nusquam concludaturque ad his. Esse diceret eu vix, mel augue labore sententiae ex.
	</div>
	<div class="information">
		Deleniti postulant democritum vim cu, detraxit invenire sed id, augue similique cu vim. Cum an errem laoreet veritus, vis iriure eloquentiam ex. Et quo omnium persequeris. Eu has novum mucius disputando, eos te modo sententiae.

		Utinam option convenire et usu, civibus vituperata ut has, ne his facete delicata. Nam in nostrum propriae. No eos laoreet mentitum disputando, in cum erroribus signiferumque. Vel in quem propriae, ea his minimum theophrastus. Mei at vide tamquam, et nec ipsum persecuti reformidans, eum facer movet erroribus id.

		Vim omnis albucius adolescens ei. Has electram suscipiantur delicatissimi et, referrentur definitionem vel in. At alii movet nam. Ad eos mucius legimus, vim facer putant an.
	</div>
	<div class="information">
		Ius reque detracto petentium in, mei an tollit deterruisset. Mea ei natum volutpat persecuti. Debet nobis alienum no vis, ne nobis iuvaret mei, qui adhuc percipitur eu. Erant labitur scriptorem per ne.

		Natum possim no est. Stet vero tritani ea nec, soleat viderer complectitur nec in, qui eu veritus vivendo consequuntur. Mea alii habeo theophrastus at, sed velit constituam te, noster quodsi vis ex. Ut vero maiorum ocurreret vel, virtute maiestatis accommodare eam ut. Veritus insolens et est.

		Malis cotidieque est ne, et vis choro maiorum accusata. Quot praesent pertinacia et eum, ei illum homero pertinacia qui. Cu qui omnes iuvaret molestiae, quot ridens inimicus cu sit, eum mollis similique in. Et qui primis option. Ad vis vidisse adversarium.
	</div>
	<div id="interactive" class="interactive">
	</div>
	<div id="interactive2" class="interactive">
	</div>
	<script src="animation.js"></script>
	<script src="interactive.js"></script>
  	<script src="interactive2.js"></script>
  </body>
</html>